![alt text](https://github.com/jamesngondo2013/Gesture-Based-UI-Development/blob/master/arduino_motor.png)
# Gesture Based UI Development

### Team Members
##### James Ngondo
##### Hary Wang
##### Julien Yaho

## Introduction
A Gesture Controlled robot is a kind of robot which can be controlled by  your hand gestures not by old buttons.
You just need to wear a small transmitting device in your hand which included an acceleration meter.
This will transmit an appropriate command to the robot so that it can do whatever we want.

## Purpose of the application 
The purpose of this application is to demonstrate how [Myo](https://www.myo.com/) armbands can be used to detect the electrical activity of our muscles. 
The Myo armband has Electromyography (EMG) sensors that directly sense muscle activity and motion, allowing it to read the activity 
of our muscles in a refined way. As a result, gesture control has become a practicable way of interacting with everyday devices such as robots, 
flying drones, control sound and lights on stage. Our robot tanker/ vehicle is an excellent example to demonstrate the merge of robotics with Myo.
This project also demonstrates how gesture controlled vehicles can used in situations which do not permit the Unmanned Ground Vehicles.

## Gestures identified as appropriate for this application 
The MYO is an armband equipped with several Electromyography (EMG) sensors that can recognize hand gestures and the movement of the arms. Based on the electrical 
impulses generated by muscles, 8 EMG sensors are responsible to recognize and perform each gesture.
For this project, Myo provides the following intuitive hand movements/ gestures:

#### 1. Wave-Out
The user waves out their wrist and this will command the robot to turn right.
#### 2. Wave-In
The user waves in their wrist and the robot turns to the left.
#### 3. Fist
Making a fist will stop the robot/ vehicle and at the same time turn on the Rdg LED light symbolizing stop.
#### 4. Fingerspread
This will command the robot to move forward and turns on the green LED lights symbolizing go forward.
#### 5. Rest
This will reset all the LED lights.

## Hardware and software used in creating the application
1. Robot tanker/ truck with two DC Motors
2. A [Myo](https://www.myo.com/) armband running the latest firmware
3. An [Arduino](https://www.arduino.cc/en/Main/ArduinoBoardUno) board like the Uno and a USB connector cable
4. [Adafruit Motorshield](http://www.adafruit.com/products/1438)
5. The Arduino [software/IDE](https://www.arduino.cc/en/Main/Software)
6. A Windows PC or Laptop, Mac, Linux
7. A bunch of LEDs.
8. A bread board
9. 4v battries
10. 2.2k ohm resistor
11. The [MyoDuino project](https://market.myo.com/app/54bd7403e4b00db53ad527a2/myoduino-). 
12. You will also need the [Microsoft Visual C++ Runtime](https://support.microsoft.com/en-us/kb/2977003), if you don't already have that installed.

## Architecture for the solution 
We hooked our Arduino board with an Adafruit Motorshield that takes two DC Motors and also supply power to the breadboard.
On the breadboard are two LED lights and resistors. An Arduino project you want to gesture-enable. 
We installed [MyoDuino](https://market.myo.com/app/54bd7403e4b00db53ad527a2/myoduino-), a simple program to enable serial communications to an Arduino. This programs sends formatted pose information to an Arduino board connected to our computer.
We have a programming script written in C (using the Arduino IDE) that actually pass the commands to the arduino board and communicates with Myo.
The Myo has been effectively synchronized with our robot to control its movements in all directions. Velocity and braking of the robot are also well controlled using MYO.
MYO is worn on the forearm and it works by using a biosensor to pick up minute electrical impulses in the user's arm muscles.
The [software development kit (SDK)](https://developer.thalmic.com/) takes care of all of the low level details related to Bluetooth connections and data transmission. At its core, 
the MYO armband provides spatial data and gestural data to an application. Spatial data informs the application about the orientation and movement 
of the user's arm. The SDK provides data to the application in the form of events. 

## Conclusions & Recommendations
This latest technology can help us minimize our dependence on huge hardware and software systems that control robots and other devices. 
This simple wearable band makes it possible for us to control and command robots using our gestures.
Looking further into the future of this Gesture Controlled Robots using Myo, we find that Myo can be very well suited to operate in military or 
defence departments to deal with the enemies hence reducing our dependency on huge external tracking systems that are time consuming to combat enemies.
We strongly believe that this project marks the start of our experiments with Myo, and we are focused on research and 
discovering new and intuitive ways that can be used to program and simultaneously control multiple skilled robots that 
can work side by side with humans and can contribute in the workplace . 

